{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9ce2cf6-540a-4eb8-8b34-6fbebd64b096",
      "metadata": {
        "id": "d9ce2cf6-540a-4eb8-8b34-6fbebd64b096"
      },
      "source": [
        "# Handwritten Number Recognition Model (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8979ac78-cd55-4d91-993f-b67156dbd116",
      "metadata": {
        "id": "8979ac78-cd55-4d91-993f-b67156dbd116"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f60ed279-2060-429f-ab0e-2488e6da8651",
      "metadata": {
        "collapsed": true,
        "id": "f60ed279-2060-429f-ab0e-2488e6da8651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f4142c-2002-43b7-bade-38aa9bbdfd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.10 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.10 pymupdf-1.24.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        }
      ],
      "source": [
        "!pip install easyocr\n",
        "!pip install pymupdf\n",
        "import io\n",
        "import PIL\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "\n",
        "from tensorflow.keras.preprocessing import image #type:ignore\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator #type:ignore\n",
        "from tensorflow.keras.datasets import mnist #type:ignore\n",
        "from tensorflow.keras.models import Sequential #type:ignore\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input #type:ignore\n",
        "from tensorflow.keras.callbacks import EarlyStopping #type:ignore\n",
        "from tensorflow.keras.models import load_model #type:ignore\n",
        "\n",
        "from google.colab.patches import cv2_imshow #type:ignore\n",
        "import easyocr\n",
        "reader = easyocr.Reader(['en'])\n",
        "import pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fa06d6-f8d1-45d9-afe0-bef8c24dccbf",
      "metadata": {
        "id": "07fa06d6-f8d1-45d9-afe0-bef8c24dccbf"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b19fc2be-f5f5-40e1-b611-ced8cf94392d",
      "metadata": {
        "id": "b19fc2be-f5f5-40e1-b611-ced8cf94392d",
        "outputId": "a6355f0c-a352-43da-ff60-795e1e41a445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train) , (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe0df75-6926-4a36-a62e-ce6428cddc9c",
      "metadata": {
        "id": "dbe0df75-6926-4a36-a62e-ce6428cddc9c"
      },
      "source": [
        "## Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fcf6c811-d2c0-46ef-b9af-54b10645b8b7",
      "metadata": {
        "id": "fcf6c811-d2c0-46ef-b9af-54b10645b8b7"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = x_train.reshape(60000, 28,28,1) , y_train.reshape(60000, 1)\n",
        "x_test, y_test = x_test.reshape(10000, 28,28,1) , y_test.reshape(10000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b05e93-ec61-4223-a05d-b1d30a433916",
      "metadata": {
        "id": "50b05e93-ec61-4223-a05d-b1d30a433916"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f772fb2-48b9-4c5c-9ef0-42ab328c00fc",
      "metadata": {
        "id": "0f772fb2-48b9-4c5c-9ef0-42ab328c00fc"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = x_train/255.0 , x_test/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba6b9bf-54ca-43e1-967e-7d05ebf37a60",
      "metadata": {
        "id": "6ba6b9bf-54ca-43e1-967e-7d05ebf37a60"
      },
      "source": [
        "## Building Model / Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a55791aa-576d-4a0a-af50-7ba639057969",
      "metadata": {
        "id": "a55791aa-576d-4a0a-af50-7ba639057969",
        "outputId": "ed1d69e9-23db-41e9-91fd-abda7b1cf17f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.6038 - loss: 1.3592 - val_accuracy: 0.9300 - val_loss: 0.2329\n",
            "Epoch 2/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9325 - loss: 0.2165 - val_accuracy: 0.9638 - val_loss: 0.1285\n",
            "Epoch 3/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9629 - loss: 0.1241 - val_accuracy: 0.9728 - val_loss: 0.0926\n",
            "Epoch 4/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9738 - loss: 0.0863 - val_accuracy: 0.9775 - val_loss: 0.0784\n",
            "Epoch 5/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9781 - loss: 0.0724 - val_accuracy: 0.9800 - val_loss: 0.0652\n",
            "Epoch 6/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9819 - loss: 0.0596 - val_accuracy: 0.9829 - val_loss: 0.0582\n",
            "Epoch 7/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9853 - loss: 0.0521 - val_accuracy: 0.9831 - val_loss: 0.0568\n",
            "Epoch 8/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9874 - loss: 0.0467 - val_accuracy: 0.9852 - val_loss: 0.0494\n",
            "Epoch 9/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9866 - loss: 0.0445 - val_accuracy: 0.9823 - val_loss: 0.0579\n",
            "Epoch 10/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9877 - loss: 0.0386 - val_accuracy: 0.9881 - val_loss: 0.0447\n",
            "Epoch 11/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9900 - loss: 0.0348 - val_accuracy: 0.9878 - val_loss: 0.0447\n",
            "Epoch 12/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9903 - loss: 0.0318 - val_accuracy: 0.9876 - val_loss: 0.0442\n",
            "Epoch 13/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9912 - loss: 0.0295 - val_accuracy: 0.9878 - val_loss: 0.0413\n",
            "Epoch 14/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9928 - loss: 0.0251 - val_accuracy: 0.9850 - val_loss: 0.0474\n",
            "Epoch 15/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9918 - loss: 0.0269 - val_accuracy: 0.9874 - val_loss: 0.0422\n",
            "Epoch 16/30\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9922 - loss: 0.0255 - val_accuracy: 0.9884 - val_loss: 0.0420\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    model = load_model('model.keras')\n",
        "except:\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(28,28,1)))\n",
        "    model.add(Conv2D(32, (3,3), activation = 'relu'))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation = 'relu'))\n",
        "    model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "    model.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])\n",
        "    early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    model.fit(x = x_train , y = y_train, validation_split = 0.2, epochs = 30, batch_size = 1000, callbacks = [early])\n",
        "    model.save('model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0402c3fe-b85e-4cb0-970a-ed36e3228490",
      "metadata": {
        "id": "0402c3fe-b85e-4cb0-970a-ed36e3228490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3f3d65-df47-4617-8d3f-50ed1fb21be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9845 - loss: 0.0463\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.036394279450178146, 0.9879000186920166]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354e5da4-972b-4884-ad19-3382e2c1e576",
      "metadata": {
        "id": "354e5da4-972b-4884-ad19-3382e2c1e576"
      },
      "source": [
        "# OCR/ CNN function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e087ea6-0290-4d42-aba8-53cd4b3f7759",
      "metadata": {
        "id": "2e087ea6-0290-4d42-aba8-53cd4b3f7759"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hand_prediction(img_obj):\n",
        "    image = img_obj\n",
        "    def remove_borders(image, border_size=5):\n",
        "        # Crop out the borders by a fixed size\n",
        "        return image[border_size:-border_size, border_size:-border_size]\n",
        "    # Remove borders from the entire image\n",
        "    image = remove_borders(image, border_size=5)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2. COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply binary thresholding to the image\n",
        "    _, thresh = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    # Find contours in the thresholded image\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Sort contours by their x position\n",
        "    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
        "\n",
        "    digits = []\n",
        "\n",
        "    global num\n",
        "    global probability\n",
        "    global n\n",
        "    num = 0\n",
        "\n",
        "    for ctr in contours:\n",
        "        num = num*10\n",
        "        # Get bounding box for each contour\n",
        "        x, y, w, h = cv2.boundingRect(ctr)\n",
        "\n",
        "        # Add padding to the bounding box\n",
        "        padding = 5  # Adjust padding as needed\n",
        "        x = max(x - padding, 0)\n",
        "        y = max(y - padding, 0)\n",
        "        w = min(w + 2 * padding, image.shape[1] - x)\n",
        "        h = min(h + 2 * padding, image.shape[0] - y)\n",
        "\n",
        "        # Extract the digit using the bounding box\n",
        "        digit = thresh[y:y+h, x:x+w]\n",
        "        # Resizing to fit model input\n",
        "        resized_digit = cv2.resize(digit, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "        # Normalization\n",
        "        digit = resized_digit / 255.0\n",
        "        r = model.predict(digit.reshape(1,28,28,1))\n",
        "\n",
        "        if np.max(r) < 0.95:\n",
        "\n",
        "            cv2_imshow(np.reshape((digit*255), (28,28,1)))\n",
        "            print(f\"Predicted:{np.argmax(r)} ({round(np.max(r)*100,2)})\")\n",
        "            print(\"Actual:\")\n",
        "            n = int(input())\n",
        "            digit = digit.reshape(1,28,28,1)\n",
        "            model.fit(digit, np.array([[n]]))\n",
        "            model.save('model.keras')\n",
        "            num = num + n\n",
        "        else:\n",
        "            num = num + np.argmax(r)\n",
        "            print(np.max(r))\n",
        "\n",
        "\n",
        "    return [num]"
      ],
      "metadata": {
        "id": "XPE0vCRDh1T-"
      },
      "id": "XPE0vCRDh1T-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7f627e7c-a601-4838-9263-b24483d39c35",
      "metadata": {
        "id": "7f627e7c-a601-4838-9263-b24483d39c35"
      },
      "source": [
        "## OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5d699a23-3c46-49ff-9b6e-bb6816159135",
      "metadata": {
        "id": "5d699a23-3c46-49ff-9b6e-bb6816159135"
      },
      "outputs": [],
      "source": [
        "def ocr_prediction(img_obj):\n",
        "    try:\n",
        "        results = reader.readtext(img_obj)\n",
        "        result_set = set()\n",
        "        if len(results) > 1:\n",
        "            for x in results[0][1]:\n",
        "                result_set.add(x)\n",
        "            for x in results[1][1]:\n",
        "                result_set.add(x)\n",
        "\n",
        "        if results:\n",
        "            return results\n",
        "        else:\n",
        "            return 'ERR'\n",
        "    except Exception as e:\n",
        "        return 'ERR'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALL PAGES\n",
        "Now for all pages, we cascadingly call all functions, in a for loop\n"
      ],
      "metadata": {
        "id": "jy-lqNZxgwcZ"
      },
      "id": "jy-lqNZxgwcZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(pdf_path):\n",
        "    pdf_document = pymupdf.open(pdf_path)\n",
        "    global rows_written\n",
        "    rows_written = 0\n",
        "\n",
        "    if not os.path.exists('outputs'):\n",
        "        os.makedirs('outputs')\n",
        "\n",
        "\n",
        "    #Clearing any past file\n",
        "    with open('outputs/output.csv', mode='w') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    with open('outputs/attention.txt', mode='w') as txt_file:\n",
        "        txt_write = txt_file.write('')\n",
        "\n",
        "    for page_number in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_number)\n",
        "        # To image (using matrix transformation)\n",
        "        zoom = 2  # Adjust zoom level as needed\n",
        "        mat = pymupdf.Matrix(zoom, zoom)\n",
        "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "\n",
        "        # pixmap to PIL Image\n",
        "        img_bytes = pix.tobytes(\"ppm\")\n",
        "        image = PIL.Image.open(io.BytesIO(img_bytes))\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        #return image\n",
        "\n",
        "        #instead lets read all pages\n",
        "\n",
        "        table = get_table(image, page_number)"
      ],
      "metadata": {
        "id": "e7nqG3BYg4JN"
      },
      "id": "e7nqG3BYg4JN",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_table(image, page_number):\n",
        "    ## Filters\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "\n",
        "    ## Contour detection\n",
        "    contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
        "    table_contour = None\n",
        "    for contour in contours:\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * perimeter, True)\n",
        "\n",
        "        if len(approx) == 4:\n",
        "            table_contour = approx\n",
        "            break\n",
        "\n",
        "    if table_contour is not None:\n",
        "        # Get bounding box coordinates\n",
        "        x, y, w, h = cv2.boundingRect(table_contour)\n",
        "\n",
        "        # Crop the table region from the original image\n",
        "        table_image = image[y:y+h, x:x+w]\n",
        "        num_rows = 0\n",
        "\n",
        "        #return table_image\n",
        "\n",
        "        extract_cells_to_csv(table_image, page_number)\n",
        "\n",
        "    else:\n",
        "        print(\"error\")"
      ],
      "metadata": {
        "id": "gQVhZ7LehOUc"
      },
      "id": "gQVhZ7LehOUc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_cells_to_csv(image, page_number, csv_filename='outputs/output.csv'):\n",
        "\n",
        "    # Determine number of rows using page height\n",
        "    # height = 181 + 59*rows\n",
        "    height = len(image)\n",
        "    num_rows = round((height - 181) / 59)\n",
        "    global rows_written\n",
        "\n",
        "    # Parameters (adjust these values as needed)\n",
        "    title_offset = 106 + (num_rows * 2)  # Offset to skip the title\n",
        "    header_row_height = 75  # Height of the header row\n",
        "    row_height = 57  # Height of each subsequent row\n",
        "\n",
        "    # Column widths (adjust these values according to your table)\n",
        "    col_names = [\"SNo.\",\"ID\",\"NAME\",\"DOB\",\"INTERVIEW MARKS\"]\n",
        "    column_widths = [67, 73, 392 + (num_rows * 2), 175, 380]\n",
        "\n",
        "    # Function to get the column boundaries\n",
        "    def get_column_boundaries(column_widths):\n",
        "        boundaries = [0]\n",
        "        for width in column_widths:\n",
        "            boundaries.append(boundaries[-1] + width)\n",
        "        return boundaries\n",
        "\n",
        "    column_boundaries = get_column_boundaries(column_widths)\n",
        "\n",
        "    # Open a CSV file to write the results\n",
        "    with open(csv_filename, mode='a', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "\n",
        "        if page_number == 0:\n",
        "          # Extract header row\n",
        "          header_row = image[title_offset:title_offset + header_row_height, :]\n",
        "          header_cells = []\n",
        "\n",
        "          # Process each cell of the header row\n",
        "          for col in range(0,5):\n",
        "              cell = header_row[:, column_boundaries[col]:column_boundaries[col + 1]]\n",
        "              cell_image = np.array(cell)\n",
        "              header_cells.append(ocr_prediction(cell_image)[0][1])\n",
        "\n",
        "          # Write the header row to the CSV\n",
        "          csv_writer.writerow(header_cells)\n",
        "\n",
        "        # Extract subsequent rows\n",
        "        for row in range(1, num_rows + 1):\n",
        "            row_start = title_offset + header_row_height + (row - 1) * row_height\n",
        "            row_end = row_start + row_height\n",
        "            table_row = image[row_start:row_end, :]\n",
        "            row_cells = []\n",
        "\n",
        "            # Process each cell of the row\n",
        "            row_cells.append(f'{row + rows_written}')\n",
        "            for col in range(1,5):\n",
        "                cell = table_row[:, column_boundaries[col]:column_boundaries[col + 1]]\n",
        "                cell_image = np.array(cell)\n",
        "                if col < 4:\n",
        "\n",
        "                   if len(ocr_prediction(cell_image)) > 1:\n",
        "                       cv2_imshow(cell_image)\n",
        "                       print(f\"PREDICTION:{ocr_prediction(cell_image)[0][1]}\")\n",
        "                       print(\"ACTUAL:\")\n",
        "                       n = str(input())\n",
        "                       row_cells.append(n)\n",
        "                   else:\n",
        "                      row_cells.append(ocr_prediction(cell_image)[0][1])\n",
        "\n",
        "                if col == 4:\n",
        "                    r = hand_prediction(cell_image)\n",
        "                    row_cells.append(r[0])\n",
        "\n",
        "            # Write the row to the CSV\n",
        "            csv_writer.writerow(row_cells)\n",
        "\n",
        "    print(f\"Written {num_rows} row(s) to '{csv_filename}'.\")\n",
        "    rows_written += num_rows"
      ],
      "metadata": {
        "id": "2KbDMN4Dhgwm"
      },
      "id": "2KbDMN4Dhgwm",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDING PROBABILITY BASED WARNING FOR CNN"
      ],
      "metadata": {
        "id": "wrc43fLghtQV"
      },
      "id": "wrc43fLghtQV"
    },
    {
      "cell_type": "code",
      "source": [
        "read_pdf(\"pdf-marked-demo.pdf\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_VOq9bn6i009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "outputId": "d27f4230-3b8e-4fdf-f906-3189a6bdcb31"
      },
      "id": "_VOq9bn6i009",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "0.99996924\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "0.99999595\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "0.9999949\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "1.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "0.9525418\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "0.9979323\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "0.9999881\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "0.9998735\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "0.9999001\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "0.9991285\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "0.9978662\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "0.9963718\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "0.998197\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "0.999074\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=408x57>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION:Royal Pradhan\n",
            "ACTUAL:\n",
            "Royal Pradhan * (RAC, Delhi)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "0.9997889\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "0.9996296\n",
            "Written 8 row(s) to 'outputs/output.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=398x57>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION:Acharya\n",
            "ACTUAL:\n",
            "Sayan Acharya *# (RAC, Delhi)\n",
            "Written 3 row(s) to 'outputs/output.csv'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}